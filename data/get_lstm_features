def get_lstm_features(self, sentence, chars2, chars2_length, d):

    if self.char_mode == 'LSTM':

        chars_embeds = self.char_embeds(chars2).transpose(0, 1)

        packed = torch.nn.utils.rnn.pack_padded_sequence(chars_embeds, chars2_length)

        lstm_out, _ = self.char_lstm(packed)

        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)

        outputs = outputs.transpose(0, 1)

        chars_embeds_temp = Variable(torch.FloatTensor(torch.zeros((outputs.size(0), outputs.size(2)))))

        if self.use_gpu:
            chars_embeds_temp = chars_embeds_temp.cuda()

        for i, index in enumerate(output_lengths):
            chars_embeds_temp[i] = torch.cat((outputs[i, index-1, :self.char_lstm_dim], outputs[i, 0, self.char_lstm_dim:]))

        chars_embeds = chars_embeds_temp.clone()

        for i in range(chars_embeds.size(0)):
            chars_embeds[d[i]] = chars_embeds_temp[i]
        
        print("LSTM done")

    if self.char_mode == 'CNN':
        print("chars2.size", chars2.size()) # ([22, 9])
        print("self.char_embeds(chars2).size", self.char_embeds(chars2).size()) # ([22, 9, 25])

        chars_embeds = self.char_embeds(chars2).unsqueeze(1)
        print("chars_embeds.size", chars_embeds.size()) # ([22, 1, 9, 25])

        ## Creating Character level representation using Convolutional Neural Netowrk
        ## followed by a Maxpooling Layer
        chars_cnn_out3 = self.char_cnn3(chars_embeds)
        print("chars_cnn_out3.size", chars_cnn_out3.size()) # ([22, 25, 11, 1])

        chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,
                                             kernel_size=(chars_cnn_out3.size(2), 1)).view(chars_cnn_out3.size(0), self.out_channels)

        print("chars_embeds.size after max pool:", chars_embeds.size()) # ([22, 25])

    ## Loading word embeddings
    embeds = self.word_embeds(sentence)
    print("embeds.size-loading", embeds.size()) # ([22, 100])

    ## We concatenate the word embeddings and the character level representation
    ## to create unified representation for each word
    embeds = torch.cat((embeds, chars_embeds), 1)
    print("embeds.size-cat", embeds.size()) # ([22, 125])

    embeds = embeds.unsqueeze(1)
    print("embeds.size-unsqueeze", embeds.size()) # ([22, 1, 125])

    ## Dropout on the unified embeddings
    embeds = self.dropout(embeds)
    print("embeds.size-dropout", embeds.size()) # ([22, 1, 125])
        
    if self.word_mode == 'LSTM':
    
        ## Word lstm
        ## Takes words as input and generates a output at each step
        lstm_out, _ = self.lstm(embeds)
        print("lstm_out before reshaping", lstm_out.size()) # ([8, 1, 400])

        ## Reshaping the outputs from the lstm layer
        lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)

        ## Dropout on the lstm output
        lstm_out = self.dropout(lstm_out)
        print("lstm_out after dropout", lstm_out.size())  # ([8, 400])

        ## Linear layer converts the ouput vectors to tag space
        lstm_feats = self.hidden2tag(lstm_out)
        print("lstm_feats.size", lstm_feats.size()) # ([8, 19])
        
    if self.word_mode == 'CNN':
#         print("sentence:", sentence)
#         print("sentence size:", sentence.size())
        
#         word_embeds = self.word_embeds(sentence) # ([sl, 100])
#         print("word_embeds size:", word_embeds.size()) # ([21, 100])
        
#         word_embeds = word_embeds.unsqueeze(1)
#         word_embeds = word_embeds.unsqueeze(1)
        
        word_embeds = embeds.unsqueeze(1)
        print("word_embeds", word_embeds.size())
        
        word_cnn_out3 = self.word_cnn3(word_embeds)
        # self.word_cnn3 = nn.Conv2d(in_channels=1, out_channels=hidden_dim*2, kernel_size=(3, self.word_embedding_dim), padding=(1,0))
        
        print("word_cnn_out3 size:", word_cnn_out3.size()) # ([21, 400, 1, 1])

        # optional max pool
        # word_embeds = nn.functional.max_pool2d(word_cnn_out3,
        #                              kernel_size=(word_cnn_out3.size(2), 1)).view(word_cnn_out3.size(0), self.hidden_dim*2)
        
        cnn_out = word_cnn_out3.squeeze(2)
        cnn_out = cnn_out.squeeze(2)
        
        cnn_out = self.dropout(cnn_out)
        
        lstm_feats = self.hidden2tag(cnn_out)

        print("lstm_feats.size", lstm_feats.size()) # ([21, 19])
    
    return lstm_feats